{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9436b4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import LeaveOneOut, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# กำหนด path ต่างๆ ไว้ล่วงหน้า\n",
    "DATA_PATH = \"data/\"  # แก้ไขเป็น path ที่เก็บไฟล์เสียงของคุณ\n",
    "OUTPUT_PATH = \"results/\"  # path ที่จะเก็บผลลัพธ์และโมเดล\n",
    "\n",
    "def extract_features(audio_file):\n",
    "    \"\"\"สกัดคุณลักษณะจากไฟล์เสียง\"\"\"\n",
    "    # โหลดไฟล์เสียง\n",
    "    y, sr = librosa.load(audio_file, sr=16000)\n",
    "    \n",
    "    # 1. MFCC คุณลักษณะ\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "    mfcc_stats = np.hstack([\n",
    "        np.mean(mfccs, axis=1),\n",
    "        np.std(mfccs, axis=1)\n",
    "    ])\n",
    "    \n",
    "    # 2. คุณลักษณะทาง spectral\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
    "    \n",
    "    spectral_stats = np.hstack([\n",
    "        [np.mean(spectral_centroids), np.std(spectral_centroids)],\n",
    "        [np.mean(spectral_rolloff), np.std(spectral_rolloff)]\n",
    "    ])\n",
    "    \n",
    "    # 3. คุณลักษณะเกี่ยวกับความดัง\n",
    "    rms = librosa.feature.rms(y=y)[0]\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y=y)[0]\n",
    "    \n",
    "    volume_stats = np.hstack([\n",
    "        [np.mean(rms), np.std(rms)],\n",
    "        [np.mean(zero_crossing_rate), np.std(zero_crossing_rate)]\n",
    "    ])\n",
    "    \n",
    "    # 4. คุณลักษณะเกี่ยวกับ pitch และ rhythm\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    tempo = librosa.beat.tempo(y=y, sr=sr)[0]\n",
    "    \n",
    "    pitch_rhythm_stats = np.hstack([\n",
    "        [tempo],\n",
    "        np.mean(chroma, axis=1)\n",
    "    ])\n",
    "    \n",
    "    # 5. คุณลักษณะเกี่ยวกับความเงียบ\n",
    "    non_silent = librosa.effects.split(y, top_db=30)\n",
    "    if len(non_silent) > 0:\n",
    "        non_silent_duration = np.sum([end - start for start, end in non_silent]) / sr\n",
    "        total_duration = len(y) / sr\n",
    "        silence_ratio = 1 - (non_silent_duration / total_duration)\n",
    "        avg_phrase_length = non_silent_duration / len(non_silent)\n",
    "    else:\n",
    "        silence_ratio = 1.0\n",
    "        avg_phrase_length = 0.0\n",
    "    \n",
    "    silence_stats = np.array([silence_ratio, avg_phrase_length, len(non_silent) / total_duration])\n",
    "    \n",
    "    # รวมคุณลักษณะทั้งหมด\n",
    "    features = np.hstack([\n",
    "        mfcc_stats,\n",
    "        spectral_stats,\n",
    "        volume_stats,\n",
    "        pitch_rhythm_stats,\n",
    "        silence_stats\n",
    "    ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def train_and_evaluate(data_path=DATA_PATH, output_path=OUTPUT_PATH):\n",
    "    \"\"\"\n",
    "    ฝึกโมเดลและวัดผลในขั้นตอนเดียว\n",
    "    \n",
    "    Args:\n",
    "        data_path: path ของโฟลเดอร์ที่เก็บไฟล์เสียง\n",
    "        output_path: path สำหรับบันทึกโมเดลและผลลัพธ์\n",
    "    \"\"\"\n",
    "    # สร้างโฟลเดอร์สำหรับบันทึกผลลัพธ์\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    print(\"= ขั้นตอนที่ 1: การโหลดและสกัดคุณลักษณะ =\")\n",
    "    \n",
    "    # โหลดไฟล์เสียงและสกัดคุณลักษณะ\n",
    "    features_list = []\n",
    "    labels = []\n",
    "    file_names = []\n",
    "    \n",
    "    # โหลดไฟล์เสียงและสกัดคุณลักษณะ\n",
    "    for file in os.listdir(data_path):\n",
    "        if file.endswith((\".wav\", \".mp3\")):\n",
    "            try:\n",
    "                # ระบุ label จากชื่อไฟล์ (ตัวอักษรตัวแรก)\n",
    "                first_char = file[0].upper()\n",
    "                if first_char == 'H':\n",
    "                    label = 'High'\n",
    "                elif first_char == 'M':\n",
    "                    label = 'Mid'\n",
    "                elif first_char == 'L':\n",
    "                    label = 'Low'\n",
    "                else:\n",
    "                    print(f\"ข้ามไฟล์ {file} เนื่องจากไม่สามารถระบุระดับได้\")\n",
    "                    continue\n",
    "                \n",
    "                # สกัดคุณลักษณะ\n",
    "                file_path = os.path.join(data_path, file)\n",
    "                features = extract_features(file_path)\n",
    "                \n",
    "                # เก็บข้อมูล\n",
    "                features_list.append(features)\n",
    "                labels.append(label)\n",
    "                file_names.append(file)\n",
    "                \n",
    "                print(f\"สกัดคุณลักษณะสำเร็จ: {file} (label: {label})\")\n",
    "            except Exception as e:\n",
    "                print(f\"เกิดข้อผิดพลาดกับไฟล์ {file}: {str(e)}\")\n",
    "    \n",
    "    # แปลงเป็น numpy array\n",
    "    X = np.array(features_list)\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(labels)\n",
    "    \n",
    "    # บันทึกข้อมูลเมตาดาต้า\n",
    "    metadata = pd.DataFrame({\n",
    "        'file': file_names,\n",
    "        'label': labels,\n",
    "        'label_encoded': y\n",
    "    })\n",
    "    metadata.to_csv(os.path.join(output_path, 'metadata.csv'), index=False)\n",
    "    \n",
    "    # ตรวจสอบสมดุลของข้อมูล\n",
    "    class_counts = pd.Series(labels).value_counts()\n",
    "    print(\"\\nการกระจายของข้อมูล:\")\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"- {cls}: {count} ไฟล์\")\n",
    "    \n",
    "    # แสดงกราฟการกระจายของข้อมูล\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.countplot(x=labels)\n",
    "    plt.title('การกระจายของระดับความสามารถ')\n",
    "    plt.xlabel('ระดับความสามารถ')\n",
    "    plt.ylabel('จำนวนไฟล์')\n",
    "    plt.savefig(os.path.join(output_path, 'data_distribution.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\n= ขั้นตอนที่ 2: การปรับพารามิเตอร์ =\")\n",
    "    \n",
    "    # พารามิเตอร์ที่ต้องการทดสอบ\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.01, 0.1],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "    \n",
    "    # มาตรฐานข้อมูล\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # ใช้ GridSearchCV หาพารามิเตอร์ที่ดีที่สุด\n",
    "    grid = GridSearchCV(\n",
    "        SVC(probability=True, class_weight='balanced'),\n",
    "        param_grid,\n",
    "        cv=5,  # 5-fold cross-validation\n",
    "        scoring='accuracy',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # เทรนโมเดล\n",
    "    grid.fit(X_scaled, y)\n",
    "    \n",
    "    # พารามิเตอร์ที่ดีที่สุด\n",
    "    best_params = grid.best_params_\n",
    "    print(f\"\\nพารามิเตอร์ที่ดีที่สุด: {best_params}\")\n",
    "    \n",
    "    # บันทึกผลการปรับพารามิเตอร์\n",
    "    cv_results = pd.DataFrame(grid.cv_results_)\n",
    "    cv_results.to_csv(os.path.join(output_path, 'grid_search_results.csv'), index=False)\n",
    "    \n",
    "    print(\"\\n= ขั้นตอนที่ 3: การวัดประสิทธิภาพด้วย Leave-One-Out Cross-Validation =\")\n",
    "    \n",
    "    # กำหนด Leave-One-Out Cross-Validation\n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    # เตรียมตัวแปรเก็บผลลัพธ์\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    file_results = []\n",
    "    \n",
    "    # ทดสอบด้วย LOOCV\n",
    "    for train_idx, test_idx in loo.split(X):\n",
    "        # แบ่งข้อมูล\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # มาตรฐานข้อมูล\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # สร้างและเทรนโมเดล SVM ด้วยพารามิเตอร์ที่ดีที่สุด\n",
    "        svm = SVC(\n",
    "            kernel=best_params['kernel'],\n",
    "            C=best_params['C'],\n",
    "            gamma=best_params['gamma'],\n",
    "            probability=True,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        svm.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # ทำนายผลลัพธ์\n",
    "        y_test_pred = svm.predict(X_test_scaled)\n",
    "        probabilities = svm.predict_proba(X_test_scaled)[0]\n",
    "        \n",
    "        # เก็บผลลัพธ์\n",
    "        y_true.append(y_test[0])\n",
    "        y_pred.append(y_test_pred[0])\n",
    "        \n",
    "        # เก็บข้อมูลไฟล์ที่ทดสอบ\n",
    "        test_file = file_names[test_idx[0]]\n",
    "        true_label = le.inverse_transform([y_test[0]])[0]\n",
    "        pred_label = le.inverse_transform([y_test_pred[0]])[0]\n",
    "        correct = (y_test[0] == y_test_pred[0])\n",
    "        \n",
    "        file_result = {\n",
    "            'file': test_file,\n",
    "            'true_label': true_label,\n",
    "            'predicted_label': pred_label,\n",
    "            'correct': correct,\n",
    "            'prob_low': probabilities[0] if len(probabilities) > 0 else 0,\n",
    "            'prob_mid': probabilities[1] if len(probabilities) > 1 else 0,\n",
    "            'prob_high': probabilities[2] if len(probabilities) > 2 else 0\n",
    "        }\n",
    "        file_results.append(file_result)\n",
    "    \n",
    "    # สร้าง DataFrame เก็บผลลัพธ์รายไฟล์\n",
    "    df_results = pd.DataFrame(file_results)\n",
    "    df_results.to_csv(os.path.join(output_path, 'leave_one_out_results.csv'), index=False)\n",
    "    \n",
    "    # ประเมินผลโมเดล\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"ความแม่นยำโดยรวม: {accuracy:.4f}\")\n",
    "    \n",
    "    # สร้าง confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # รายงานการจำแนกประเภท\n",
    "    report = classification_report(y_true, y_pred, target_names=le.classes_, output_dict=True)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    for cls in le.classes_:\n",
    "        precision = report[cls]['precision']\n",
    "        recall = report[cls]['recall']\n",
    "        f1 = report[cls]['f1-score']\n",
    "        support = report[cls]['support']\n",
    "        print(f\"- {cls}: Precision={precision:.2f}, Recall={recall:.2f}, F1={f1:.2f}, Support={support}\")\n",
    "    \n",
    "    # แสดง confusion matrix ในรูปแบบกราฟ\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=le.classes_, \n",
    "                yticklabels=le.classes_)\n",
    "    plt.xlabel('ค่าทำนาย')\n",
    "    plt.ylabel('ค่าจริง')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(os.path.join(output_path, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\n= ขั้นตอนที่ 4: การสร้างและบันทึกโมเดลสุดท้าย =\")\n",
    "    \n",
    "    # สร้างโมเดลสุดท้ายด้วยชุดข้อมูลทั้งหมด\n",
    "    final_model = SVC(\n",
    "        kernel=best_params['kernel'],\n",
    "        C=best_params['C'],\n",
    "        gamma=best_params['gamma'],\n",
    "        probability=True,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    # เทรนโมเดลด้วยข้อมูลทั้งหมด\n",
    "    final_model.fit(X_scaled, y)\n",
    "    \n",
    "    # บันทึกโมเดลและองค์ประกอบที่จำเป็น\n",
    "    os.makedirs(os.path.join(output_path, 'model'), exist_ok=True)\n",
    "    joblib.dump(final_model, os.path.join(output_path, 'model', 'svm_model.pkl'))\n",
    "    joblib.dump(scaler, os.path.join(output_path, 'model', 'scaler.pkl'))\n",
    "    joblib.dump(le, os.path.join(output_path, 'model', 'label_encoder.pkl'))\n",
    "    \n",
    "    print(f\"\\nสร้างและบันทึกโมเดลเรียบร้อยแล้ว ที่ {os.path.join(output_path, 'model')}\")\n",
    "    print(f\"ความแม่นยำของโมเดล: {accuracy:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model_path': os.path.join(output_path, 'model'),\n",
    "        'accuracy': accuracy,\n",
    "        'best_params': best_params\n",
    "    }\n",
    "\n",
    "# เรียกใช้ฟังก์ชันโดยตรง (ไม่ต้องรับ input)\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"กำลังเทรนโมเดลจากข้อมูลใน: {DATA_PATH}\")\n",
    "    print(f\"จะบันทึกผลลัพธ์ไปยัง: {OUTPUT_PATH}\")\n",
    "    \n",
    "    try:\n",
    "        result = train_and_evaluate()\n",
    "        print(\"\\nการฝึกโมเดลเสร็จสิ้น\")\n",
    "        print(f\"โมเดลถูกบันทึกที่: {result['model_path']}\")\n",
    "        print(f\"ความแม่นยำ: {result['accuracy']:.4f}\")\n",
    "        print(f\"พารามิเตอร์ที่ดีที่สุด: {result['best_params']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"เกิดข้อผิดพลาด: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thstd2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
